# ──────────────────────────────────────────────────────────────────────────────
# cloud-native-app default values
# Override any value with:  helm install ... --set key=value
#                     or:  helm install ... -f my-values.yaml
# ──────────────────────────────────────────────────────────────────────────────

# ---------------------------------------------------------------------------
# Global
# ---------------------------------------------------------------------------
namespace: cloud-native-metrics

# ---------------------------------------------------------------------------
# Frontend
# ---------------------------------------------------------------------------
frontend:
  replicaCount: 1
  image:
    repository: tasb/training-frontend
    tag: "traces"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 80
  resources:
    requests:
      cpu: 50m
      memory: 32Mi
    limits:
      cpu: 100m
      memory: 64Mi

# ---------------------------------------------------------------------------
# Backend
# ---------------------------------------------------------------------------
backend:
  replicaCount: 1
  image:
    repository: tasb/training-backend
    tag: "traces"           # includes OpenTelemetry instrumentation + richer spans
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 3000
  resources:
    requests:
      cpu: 100m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 128Mi
  # OpenTelemetry configuration (env vars injected into the container)
  otel:
    serviceName: backend-api
    tracesSampler: parentbased_always_on

# ---------------------------------------------------------------------------
# Database (PostgreSQL)
# ---------------------------------------------------------------------------
database:
  image:
    repository: postgres
    tag: "15-alpine"
    pullPolicy: IfNotPresent
  replicaCount: 1
  config:
    name: cloudnative
    user: postgres
  secret:
    # Base64 of "postgres" — change in production!
    passwordBase64: cG9zdGdyZXM=
  service:
    type: ClusterIP
    port: 5432
  resources:
    requests:
      cpu: 250m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

# ---------------------------------------------------------------------------
# Ingress
# ---------------------------------------------------------------------------
ingress:
  enabled: true
  className: nginx
  host: training.metrics.local
  annotations: {}

# ──────────────────────────────────────────────────────────────────────────────
# Sub-chart: Prometheus
# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus
# ──────────────────────────────────────────────────────────────────────────────
prometheus:
  enabled: true
  alertmanager:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  kube-state-metrics:
    enabled: true
  nodeExporter:
    enabled: false
  server:
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
  serverFiles:
    prometheus.yml:
      scrape_configs:
        # Prometheus scrapes itself
        - job_name: prometheus
          static_configs:
            - targets: ["localhost:9090"]

        # Scrape the backend /metrics endpoint (custom + OTel metrics)
        - job_name: backend-api
          scrape_interval: 10s
          metrics_path: /metrics
          static_configs:
            - targets: ["backend-service.cloud-native-metrics.svc.cluster.local:3000"]
          relabel_configs:
            - source_labels: [__address__]
              target_label: instance

        # kube-state-metrics for pod/deployment health panels in Grafana
        # NOTE: service name = <release-name>-kube-state-metrics (assumes release = "training")
        - job_name: kube-state-metrics
          static_configs:
            - targets: ["training-kube-state-metrics.cloud-native-metrics.svc.cluster.local:8080"]

# ──────────────────────────────────────────────────────────────────────────────
# Sub-chart: Grafana
# https://github.com/grafana/helm-charts/tree/main/charts/grafana
# ──────────────────────────────────────────────────────────────────────────────
grafana:
  enabled: true
  adminUser: admin
  adminPassword: admin123   # Change in production

  service:
    type: NodePort
    nodePort: 32500

  # Pre-configure datasources so you don't have to click through the UI.
  # NOTE: values.yaml is NOT templated, so URLs are literal strings.
  # These assume the Helm release name is "training" and namespace is "cloud-native-metrics".
  # If you use a different release name, update these URLs accordingly.
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          uid: prometheus
          url: http://training-prometheus-server.cloud-native-metrics.svc.cluster.local
          access: proxy
          isDefault: true
          jsonData:
            timeInterval: "15s"

        - name: Jaeger
          type: jaeger
          uid: jaeger
          url: http://jaeger.cloud-native-metrics.svc.cluster.local:16686
          access: proxy

  # Dashboard providers — load from a ConfigMap mounted at /var/lib/grafana/dashboards
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: Cloud Native Training
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default

  # Reference the ConfigMap created by our chart (templates/grafana-dashboard-cm.yaml).
  # Assumes release name = "training". Update if you use a different release name.
  dashboardsConfigMaps:
    default: "training-grafana-dashboards"

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

# ──────────────────────────────────────────────────────────────────────────────
# Jaeger all-in-one (deployed via templates/jaeger/, NOT a Helm sub-chart)
# The jaegertracing/jaeger chart v4+ requires persistent storage (Cassandra /
# Elasticsearch). We use the all-in-one image with in-memory storage instead.
# Backend sends OTLP/gRPC spans to port 4317. UI is on port 16686.
# ──────────────────────────────────────────────────────────────────────────────
jaeger:
  enabled: true
  image:
    repository: jaegertracing/all-in-one
    tag: "1.57.0"
    pullPolicy: IfNotPresent
  maxTraces: 50000
  service:
    type: ClusterIP      # access via: kubectl port-forward svc/jaeger 16686:16686
    uiNodePort: 32001    # only used when type is NodePort
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

# ──────────────────────────────────────────────────────────────────────────────
# HPA — scales backend based on db_items_total
# Formula: desiredReplicas = ceil(db_items_total / itemsPerReplica)
# Examples: 1 pod for 0-10 items, 2 pods for 11-20, 3 pods for 21-30, etc.
# ──────────────────────────────────────────────────────────────────────────────
hpa:
  enabled: true
  minReplicas: 1
  maxReplicas: 10
  itemsPerReplica: 10   # 1 replica per N items in DB

# ──────────────────────────────────────────────────────────────────────────────
# Sub-chart: prometheus-adapter
# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter
# Bridges Prometheus metrics into the Kubernetes External Metrics API so HPA
# can consume db_items_total.
# NOTE: prometheus.url assumes release name "training" and namespace "cloud-native-metrics".
# ──────────────────────────────────────────────────────────────────────────────
prometheus-adapter:
  enabled: true
  prometheus:
    url: http://training-prometheus-server.cloud-native-metrics.svc.cluster.local
    port: 80
  rules:
    external:
      - seriesQuery: 'db_items_total{job="backend-api"}'
        name:
          as: "db_items_total"
        metricsQuery: 'max(db_items_total{job="backend-api"})'
